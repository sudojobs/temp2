{
  "server": {
    "host": "0.0.0.0",
    "port": 8099
  },
  "llm": {
    "adapter": "llama_cpp",
    "model_path": "models/Meta-Llama-3.1-8B-Instruct-Q4_K_L.gguf",
    "temperature": 0.1,
    "max_tokens": 1024,
    "n_ctx": 4096,
    "threads": 8,
    "gpu_layers": 35
  },
  "embedding": {
    "model": "BAAI/bge-base-en-v1.5",
    "local_path": "models/bge-base-en-v1.5",
    "dim": 1024,
    "batch_size": 32
  }
}
